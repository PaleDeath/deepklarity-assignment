# Copy this file to .env and fill in values.

# LLM provider: "gemini" or "groq"
LLM_PROVIDER=gemini

# Primary Gemini key (Google AI Studio key)
GEMINI_API_KEY=your_gemini_api_key_here

# Optional: comma-separated keys for automatic fallback rotation.
# GEMINI_API_KEYS=key1,key2,key3

# Optional: model fallback order. First model is tried first.
# Keep lower-cost/free-tier friendly models first.
GEMINI_MODELS=gemini-1.5-flash,gemini-2.0-flash

# Optional: retries per key/model on transient quota responses.
GEMINI_MAX_RETRIES=3

# Groq key and model fallback (used when LLM_PROVIDER=groq)
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODELS=llama-3.3-70b-versatile,llama-3.1-8b-instant

# PostgreSQL connection string
DATABASE_URL=postgresql://postgres:password@localhost:5432/wiki_quiz

# CORS: comma-separated origins (for production, add your Vercel URL)
# ALLOWED_ORIGINS=https://your-app.vercel.app
